{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers, losses\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Bidirectional,Input\n",
    "from tensorflow.keras.layers import BatchNormalization, TimeDistributed,LSTM,Dropout,Reshape\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "from data import preproc as pp\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = '/home/austin/Documents/Github/handwritingnotebook/data/'\n",
    "data_dir = '/Documents/handwritingnotebook/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = pd.read_csv(data_dir+'words_csv/2020-06-03 11:39:42.000901.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "IMG_HEIGHT = data_csv['height'].max()\n",
    "IMG_WIDTH = data_csv['width'].max()\n",
    "DATASET_SIZE = data_csv.shape[0]\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz.,()?!':; \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_practice = data_csv.truth.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = tf.data.TextLineDataset(data_dir+'words_csv/2020-06-03 11:39:42.000901.csv').skip(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_onehot_old(data):\n",
    "    #Creates a dict, that maps to every char of alphabet an unique int based on position\n",
    "    char_to_int = dict((c,i) for i,c in enumerate(alphabet))\n",
    "    encoded_data = []\n",
    "    #Replaces every char in data with the mapped int\n",
    "    encoded_data.append([char_to_int[char] for char in data])\n",
    "    encoded_data = encoded_data[0] # Prints the int encoded array\n",
    "\n",
    "    #This part now replaces the int by an one-hot array with size alphabet\n",
    "    letter = [0. for _ in range(len(alphabet)+1)]\n",
    "    for value in encoded_data:\n",
    "        #At first, the whole array is initialized with 0\n",
    "        #Only at the number of the int, 1 is written\n",
    "        letter[value] = 1.\n",
    "    letter = tf.convert_to_tensor(letter)\n",
    "    return letter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_onehot(data):\n",
    "    #Creates a dict, that maps to every char of alphabet an unique int based on position\n",
    "    char_to_int = dict((c,i) for i,c in enumerate(alphabet))\n",
    "    encoded_data = []\n",
    "    #Replaces every char in data with the mapped int\n",
    "    encoded_data.append([char_to_int[char] for char in data])\n",
    "    encoded_data = encoded_data[0] # Prints the int encoded array\n",
    "    encoded_data += [-1] * (32 - len(encoded_data))\n",
    "    return(encoded_data)\n",
    "#     #This part now replaces the int by an one-hot array with size alphabet\n",
    "#     letter = [0. for _ in range(len(alphabet)+1)]\n",
    "#     for value in encoded_data:\n",
    "#         #At first, the whole array is initialized with 0\n",
    "#         #Only at the number of the int, 1 is written\n",
    "#         letter[value] = 1.\n",
    "#     letter = tf.convert_to_tensor(letter)\n",
    "#     return letter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path, data_dir, imgSize):\n",
    "    img_path = data_dir + 'words_screenshot_labeled/' + img_path\n",
    "    img = cv2.imread(img_path,0)\n",
    "    (wt, ht) = imgSize\n",
    "    (h, w) = img.shape\n",
    "    fx = w / wt\n",
    "    fy = h / ht\n",
    "    f = max(fx, fy)\n",
    "    newSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1)) # scale according to f (result at least 1 and at most wt or ht)\n",
    "    img = cv2.resize(img, newSize)\n",
    "    target = np.ones([ht, wt]) * 255\n",
    "    target[0:newSize[1], 0:newSize[0]] = img\n",
    "\n",
    "    # transpose for TF\n",
    "    img = cv2.transpose(target)\n",
    "\n",
    "    # normalize\n",
    "    (m, s) = cv2.meanStdDev(img)\n",
    "    m = m[0][0]\n",
    "    s = s[0][0]\n",
    "    img = img - m\n",
    "    img = img / s if s>0 else img\n",
    "    img = tf.convert_to_tensor(img)\n",
    "    img = tf.expand_dims(img,2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img_dataset(dataset,data_dir):\n",
    "    img_dataset_list = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        img_dataset_list.append(preprocess_image(row[0],data_dir,[128,32]))\n",
    "    data_as_dataset = tf.data.Dataset.from_tensor_slices(img_dataset_list)\n",
    "    return data_as_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_dataset(dataset):\n",
    "    label_dataset_list = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        label_dataset_list.append(convert_to_onehot(row[1]))\n",
    "    data_as_dataset = tf.data.Dataset.from_tensor_slices(label_dataset_list)\n",
    "    return data_as_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = create_img_dataset(data_csv,data_dir)\n",
    "labels = create_label_dataset(data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ds = tf.data.Dataset.zip((imgs,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.90 * DATASET_SIZE)\n",
    "test_size = int(0.10 * DATASET_SIZE)\n",
    "#full_dataset = labeled_ds.shuffle(DATASET_SIZE)\n",
    "full_dataset = labeled_ds\n",
    "train_ds = full_dataset.take(train_size)\n",
    "train_ds = train_ds.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_ds = full_dataset.skip(train_size)\n",
    "test_ds = test_ds.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(name=\"input\", shape=(128,32,1))\n",
    "\n",
    "cnn = layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(inputs)\n",
    "cnn = layers.BatchNormalization()(cnn)\n",
    "cnn = layers.LeakyReLU(alpha=0.01)(cnn)\n",
    "cnn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(cnn)\n",
    "\n",
    "cnn = layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n",
    "cnn = layers.BatchNormalization()(cnn)\n",
    "cnn = layers.LeakyReLU(alpha=0.01)(cnn)\n",
    "cnn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(cnn)\n",
    "\n",
    "cnn = layers.Dropout(rate=0.2)(cnn)\n",
    "cnn = layers.Conv2D(filters=48, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n",
    "cnn = layers.BatchNormalization()(cnn)\n",
    "cnn = layers.LeakyReLU(alpha=0.01)(cnn)\n",
    "cnn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(cnn)\n",
    "\n",
    "cnn = layers.Dropout(rate=0.2)(cnn)\n",
    "cnn = layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n",
    "cnn = layers.BatchNormalization()(cnn)\n",
    "cnn = layers.LeakyReLU(alpha=0.01)(cnn)\n",
    "\n",
    "cnn = layers.Dropout(rate=0.2)(cnn)\n",
    "cnn = layers.Conv2D(filters=80, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n",
    "cnn = layers.BatchNormalization()(cnn)\n",
    "cnn = layers.LeakyReLU(alpha=0.01)(cnn)\n",
    "\n",
    "shape = cnn.get_shape()\n",
    "blstm = layers.Reshape((shape[1], shape[2] * shape[3]))(cnn)\n",
    "\n",
    "blstm = layers.Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "blstm = layers.Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "blstm = layers.Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "blstm = layers.Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "blstm = layers.Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "\n",
    "blstm = layers.Dropout(rate=0.5)(blstm)\n",
    "outputs = layers.Dense(units=63, activation=\"softmax\")(blstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs = inputs, outputs = outputs, name = 'handwriting_ctc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_lambda_func(y_true, y_pred):\n",
    "        \"\"\"Function for computing the CTC loss\"\"\"\n",
    "\n",
    "        if len(y_true.shape) > 2:\n",
    "            y_true = tf.squeeze(y_true)\n",
    "            \n",
    "        input_length = tf.math.reduce_sum(y_pred, axis=-1, keepdims=False)\n",
    "        input_length = tf.math.reduce_sum(input_length, axis=-1, keepdims=True)\n",
    "        label_length = tf.math.count_nonzero(tf.greater_equal(y_true,0), axis=-1, keepdims=True, dtype=\"int64\")\n",
    "        loss = K.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss = ctc_loss_lambda_func,\n",
    "              optimizer= optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_length = []\n",
    "for images, labels in test_ds:\n",
    "    numpy_labels = labels.numpy()\n",
    "    for label in numpy_labels:\n",
    "        label_length = np.sum(label >= 0)\n",
    "        input_length.append(label_length)\n",
    "input_length = tf.convert_to_tensor(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = K.ctc_decode(out,input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
